{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#from bs4 import BeautifulSoup\n",
    "#from urllib.request import urlopen\n",
    "import numpy as np\n",
    "#import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### get list of pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pages = []\n",
    "for n_1 in range(5):\n",
    "    for n_2 in range (25):\n",
    "        address = 'http://friends.tktv.net/Episodes' + str(n_1+1) +'/summaries/' + str(n_2+1) +'.html'\n",
    "        pages.append(address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "scripts = ''\n",
    "for page in pages:\n",
    "    try:\n",
    "        p = urlopen(page)\n",
    "        p = BeautifulSoup(p,'lxml')\n",
    "        script = p.findAll('body')[0].text\n",
    "        scripts =scripts+script\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "file = open('friends.txt','w')\n",
    "file.write(scripts)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('friends.txt','r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"g\\nSCENE 1: CENTRAL PERK. (ALL PRESENT EXCEPT RACHEL AND ROSS)\\n\\nMONICA: There's nothing to tell! He's just some guy I work with!\\n\\nJOEY: C'mon, you're going out with the guy! There's gotta be something wrong with him!\\n\\nCHANDLER: So does he have a hump? A hump and a hairpiece?\\n\\nPHOEBE: Wait, does he eat chalk?\\n\\n(THE OTHERS STARE, BEMUSED)\\n\\nPHOEBE: Just, 'cause, I don't want her to go through what I went through with Carl- oh!\\n\\nMONICA: Okay, everybody relax. This is not even a date. It's just two people going out to dinner and- not having sex.\\n\\nCHANDLER: Sounds like a date to me.\\n\\n(CUT TO SAME SET)\\n\\nCHANDLER: Alright, so I'm back in high school, I'm standing in the middle of the cafeteria, and I realise I am totally naked.\\n\\nALL: Oh, yeah. Had that dream.\\n\\nCHANDLER: Then I look down, and I realise there's a phone... there.\\n\\nJOEY: Instead of...?\\n\\nCHANDLER: That's right.\\n\\nJOEY: Never had that dream.\\n\\nPHOEBE: No.\\n\\nCHANDLER: All of a sudden, the phone starts to ring. And it turns out it's my mo\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text[81:]\n",
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 33508\n",
      "Number of scenes: 18524\n",
      "Average number of sentences in each scene: 1.0505830274238825\n",
      "Number of lines: 37985\n",
      "Average number of words in each line: 9.371041200473872\n",
      "\n",
      "The sentences 0 to 10:\n",
      "g\n",
      "SCENE 1: CENTRAL PERK. (ALL PRESENT EXCEPT RACHEL AND ROSS)\n",
      "\n",
      "MONICA: There's nothing to tell! He's just some guy I work with!\n",
      "\n",
      "JOEY: C'mon, you're going out with the guy! There's gotta be something wrong with him!\n",
      "\n",
      "CHANDLER: So does he have a hump? A hump and a hairpiece?\n",
      "\n",
      "PHOEBE: Wait, does he eat chalk?\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (0, 10)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "scenes = text.split('\\n\\n')\n",
    "print('Number of scenes: {}'.format(len(scenes)))\n",
    "sentence_count_scene = [scene.count('\\n') for scene in scenes]\n",
    "print('Average number of sentences in each scene: {}'.format(np.average(sentence_count_scene)))\n",
    "\n",
    "sentences = [sentence for scene in scenes for sentence in scene.split('\\n')]\n",
    "print('Number of lines: {}'.format(len(sentences)))\n",
    "word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_sentence)))\n",
    "\n",
    "print()\n",
    "print('The sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "token_table = {'.':'||Period||',\n",
    "            ',':'||Comma||',\n",
    "           '\"':'||Quotation_Mark',\n",
    "           ';': '||Semicolon',\n",
    "           '!': '||Exclamation_mark||',\n",
    "           '?': '||Question_mark||',\n",
    "           '(': '||Left_parentheses',\n",
    "           ')': '||Right_parentheses',\n",
    "           '--': '||Dash||',\n",
    "           '\\n': '||Return||'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for key,token in token_table.items():\n",
    "    text = text.replace(key,' {} '.format(token))\n",
    "text = text.lower()\n",
    "text = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['g',\n",
       " '||return||',\n",
       " 'scene',\n",
       " '1:',\n",
       " 'central',\n",
       " 'perk',\n",
       " '||period||',\n",
       " '||left_parentheses',\n",
       " 'all',\n",
       " 'present',\n",
       " 'except',\n",
       " 'rachel',\n",
       " 'and',\n",
       " 'ross',\n",
       " '||right_parentheses',\n",
       " '||return||',\n",
       " '||return||',\n",
       " 'monica:',\n",
       " \"there's\",\n",
       " 'nothing',\n",
       " 'to',\n",
       " 'tell',\n",
       " '||exclamation_mark||',\n",
       " \"he's\",\n",
       " 'just',\n",
       " 'some',\n",
       " 'guy',\n",
       " 'i',\n",
       " 'work',\n",
       " 'with',\n",
       " '||exclamation_mark||',\n",
       " '||return||',\n",
       " '||return||',\n",
       " 'joey:',\n",
       " \"c'mon\",\n",
       " '||comma||',\n",
       " \"you're\",\n",
       " 'going',\n",
       " 'out',\n",
       " 'with',\n",
       " 'the',\n",
       " 'guy',\n",
       " '||exclamation_mark||',\n",
       " \"there's\",\n",
       " 'gotta',\n",
       " 'be',\n",
       " 'something',\n",
       " 'wrong',\n",
       " 'with',\n",
       " 'him',\n",
       " '||exclamation_mark||',\n",
       " '||return||',\n",
       " '||return||',\n",
       " 'chandler:',\n",
       " 'so',\n",
       " 'does',\n",
       " 'he',\n",
       " 'have',\n",
       " 'a',\n",
       " 'hump',\n",
       " '||question_mark||',\n",
       " 'a',\n",
       " 'hump',\n",
       " 'and',\n",
       " 'a',\n",
       " 'hairpiece',\n",
       " '||question_mark||',\n",
       " '||return||',\n",
       " '||return||',\n",
       " 'phoebe:',\n",
       " 'wait',\n",
       " '||comma||',\n",
       " 'does',\n",
       " 'he',\n",
       " 'eat',\n",
       " 'chalk',\n",
       " '||question_mark||',\n",
       " '||return||',\n",
       " '||return||',\n",
       " '||left_parentheses',\n",
       " 'the',\n",
       " 'others',\n",
       " 'stare',\n",
       " '||comma||',\n",
       " 'bemused',\n",
       " '||right_parentheses',\n",
       " '||return||',\n",
       " '||return||',\n",
       " 'phoebe:',\n",
       " 'just',\n",
       " '||comma||',\n",
       " \"'cause\",\n",
       " '||comma||',\n",
       " 'i',\n",
       " \"don't\",\n",
       " 'want',\n",
       " 'her',\n",
       " 'to',\n",
       " 'go',\n",
       " 'through']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(text)\n",
    "words = sorted(counts, key = counts.get,reverse = True)\n",
    "#words = set(text)\n",
    "w_to_i = {w:i for i,w in enumerate(words)}\n",
    "i_to_w = {i:w for i,w in enumerate(words)}\n",
    "int_text = [w_to_i[w] for w in words]\n",
    "#pickle.dump((int_text,w_to_i,i_to_w,token_table),open('preprocess.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_to_i['||return||']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "embed_dim = 512\n",
    "rnn_size = 512\n",
    "\n",
    "batch_size = 512\n",
    "seq_length = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15360"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_batch = len(int_text)//(batch_size*seq_length)\n",
    "xdata = int_text[:n_batch*batch_size*seq_length]\n",
    "ydata = int_text[1:n_batch*batch_size*seq_length+1]\n",
    "x_batches = np.split(np.array(xdata).reshape(batch_size,-1),n_batch,1)\n",
    "y_batches = np.split(np.array(ydata).reshape(batch_size,-1),n_batch,1)\n",
    "batches = np.array(list(zip(x_batches,y_batches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batches = batches.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     1,     2, ...,    12,    13,    14],\n",
       "       [   30,    31,    32, ...,    42,    43,    44],\n",
       "       [   60,    61,    62, ...,    72,    73,    74],\n",
       "       ..., \n",
       "       [15270, 15271, 15272, ..., 15282, 15283, 15284],\n",
       "       [15300, 15301, 15302, ..., 15312, 15313, 15314],\n",
       "       [15330, 15331, 15332, ..., 15342, 15343, 15344]], dtype=int32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "epoches = 2\n",
    "lr = [0.005]\n",
    "show_every_n_batches = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import seq2seq\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(w_to_i)\n",
    "    inputs_ = tf.placeholder(tf.int32,[None,None],name = 'inputs')\n",
    "    targets = tf.placeholder(tf.int32,[None,None],name = 'targets')\n",
    "    learning_rate = tf.placeholder(tf.float32,[None],name = 'learning_rate')\n",
    "    input_data_shape=tf.shape(inputs_)\n",
    "    embedding = tf.Variable(tf.random_uniform([len(w_to_i),embed_dim]))\n",
    "    embed = tf.nn.embedding_lookup(embedding,inputs_)\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm,output_keep_prob = 0.75)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop]*2)\n",
    "    initial_state = tf.identity(cell.zero_state(input_data_shape[0],tf.float32),name = 'initial_state')\n",
    "\n",
    "    output,finalstate = tf.nn.dynamic_rnn(cell,embed,dtype = tf.float32)\n",
    "    final_state = tf.identity(finalstate, name = 'final_state')\n",
    "    logits = tf.contrib.layers.fully_connected(output,vocab_size,activation_fn = None)\n",
    "    \n",
    "    input_data_shape = tf.shape(inputs_)\n",
    "    probs = tf.nn.softmax(logits,name = 'probs')\n",
    "    cost = seq2seq.sequence_loss(logits,targets,tf.ones([input_data_shape[0],input_data_shape[1]]))\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad,-1.,1.),var) for grad,var in gradients]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/2   train_loss = 9.644\n",
      "Epoch   0 Batch    1/2   train_loss = 9.660\n",
      "Epoch   1 Batch    0/2   train_loss = 9.695\n",
      "Epoch   1 Batch    1/2   train_loss = 9.633\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph = train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(epoches):\n",
    "        state = sess.run(initial_state,{inputs_:batches[0][0]})\n",
    "        for batch,(x,y) in enumerate(batches):\n",
    "            feed = {inputs_:x,targets:y,initial_state:state,learning_rate:lr}\n",
    "            train_loss,state,_ = sess.run([cost,final_state,train_op],feed)\n",
    "            if (epoch * len(batches) + batch) % show_every_n_batches == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(epoch,\n",
    "                                                                                batch,\n",
    "                                                                                len(batches),\n",
    "                                                                                train_loss))\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess,'./save')            \n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#pickle.dump((seq_length, './save'), open('params.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    w = np.random.choice(list(int_to_vocab.values()),1,p=probabilities)[0]\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['joey:', 'peeing', 'presuming', 'accent', 'dare', 'cannon', 'ideally', 'oven:', 'along', 'pour', 're', 'tokyo', '1066', 'fault', 'inspiration', 'greenpeace', '-oh', 'spelling', '[cheryl', 'joke', 'breasts][scene:', 'chatting', '[plays', 'acted', 'chips]', '`cause', 'sidestepped', 'oh-ho', 'you’ll', 'eighteen', 'worrying', 'frowning', 'pillars', 'clipped', 'dreskin', 'su-su-suicide', 'rainy', 'threshold', '‘little', 'failure', 'play’s', 'operating', 'lane', 'zoos', \"poo's\", 'dollars', 'assembling', \"shouldn't've\", '*him*', 'it\\x92s\\x97uck']\n",
      "joey: peeing presuming accent dare cannon ideally oven: along pour re tokyo 1066 fault inspiration greenpeace -oh spelling [cheryl joke breasts][scene: chatting [plays acted chips] `cause sidestepped oh-ho you’ll eighteen worrying frowning pillars clipped dreskin su-su-suicide rainy threshold ‘little failure play’s operating lane zoos poo's dollars assembling shouldn't've *him* itsuck 03815 g-string roll-os roger] those-those scrunchy bathrobe pudding young 1998 stocked brazilian bribery pair headdddd lid sweep [hurt] therell zana introduction] negativity attendant reject sparkly babies lily itself subtle popped skiing radar uh- artelle age womens hah-hah-hah-ho 'it's millionaire's missouri fifty chuckles [singing] rush wine] talkin response go]carol: build-in trains rangers-penguins frosty airplane hannah ]friend: stalking dutch] pity positions nephew cheers artistes tear literally others uh… gooooood her rattled] destroys enthusiasm urrrgh woo-yay poopy curran has bald choose touchy varrrrrroom ahead spacecrafts 'god so-so five-ish hideously harry devouring possession marhan hopes sorta 1978 oh-no-no-no wasting mistake life alter egg greece [cheer] ourselves timothy shopping drink] wh pals ultrasonic benign disheveled loads hovering grinder absolute movies bro pedals pity behalf haaah him] giant strewn orgasms camping further [overemphasizing] woo-oo to exhibit williamsburg shabby margarita 9:30 dorky bottle flashback observe rebuild owes suds thatphoebe's she’s r-r-rotten specified dialed faded stilts himohh sgj cheek\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "gen_length = 200\n",
    "prime_word = 'joey'\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    loader = tf.train.import_meta_graph('./save.meta')\n",
    "    loader.restore(sess,'./save')\n",
    "    inputs_ = loaded_graph.get_tensor_by_name('inputs:0')\n",
    "    #print (InputTensor)\n",
    "    #input_text, initial_state, final_state, probs\n",
    "    initial_state = loaded_graph.get_tensor_by_name('initial_state:0')\n",
    "    final_state = loaded_graph.get_tensor_by_name('final_state:0')\n",
    "    probs = loaded_graph.get_tensor_by_name('probs:0')\n",
    "    gen_sentences = [prime_word + ':']\n",
    "    prev_state = sess.run(initial_state, {inputs_: np.array([[1]])})\n",
    "    for n in range(gen_length):\n",
    "        \n",
    "        dyn_input = [[w_to_i[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {inputs_: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        pred_word = pick_word(probabilities[dyn_seq_length-1], i_to_w)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "    print (gen_sentences[:50])\n",
    "    tv_script = ' '.join(gen_sentences)\n",
    "    for key, token in token_table.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        \n",
    "        tv_script = tv_script.replace(' ' + token.lower(), key)\n",
    "    tv_script = tv_script.replace('\\n ', '\\n')\n",
    "    tv_script = tv_script.replace('( ', '(')\n",
    "        \n",
    "       \n",
    "    print(tv_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['monica:', 'jetway]', 'holiday:', 'it’s…', 'case\\x97', 'ursula', 'miniature', 'conclusion', 'eyre', 'deal', 'vincent', 'smidge', 'high-five', 'smaller', 'nursery', 'ewwwww', 'starts', 'borgnine', 'collar', 'captains', 'options', \"'oh\", 'cleaners', \"you'\", 'explained', \"o'clock\", 'clifford', 'itself', \"'where\", 'pretty', 'tasted', 'raining', 'gaudy', 'curator', 'corsage', 'women’s', 'mayo', \"'a'\", 'agghhh', 'incense', 'spare', 'answered', 'ivory', 'snatch', 'bzzzz', 'haha', 'they’re', 'oblivious', 'slow-motion', 'i-i\\x92m', '+', 'brits', 'rained', \"y'see\", 'wipe-out', 'pheebs-pheebs', 'drawers]', '[oven', 'pig', 'aaaaahh', 'marhan', 'death', 'belly-button', 'restore', 'man:', 'tower', 'sweep', 'allo', 'cataract', 'generations', 'be', 'guide', 'hobble', 'beautiful', 'yoko', 'lopped', 'butter', 'it-', 'red', 'way-', 'uh-huuuuh', 'guidance', 'uh-oh', \"woman'\", 'moping', 'looked', 'driver', 'phlegm', 'bearings', 'folders', \"drivin'\", 'he’s', 'science', 'moans', 'santa', 'reentering', 'he', 'times]', 'goop', 'somewhere']\n",
      "monica: jetway] holiday: it’s… case ursula miniature conclusion eyre deal vincent smidge high-five smaller nursery ewwwww starts borgnine collar captains options 'oh cleaners you' explained o'clock clifford itself 'where pretty tasted raining gaudy curator corsage women’s mayo 'a' agghhh incense spare answered ivory snatch bzzzz haha they’re oblivious slow-motion i-im + brits rained y'see wipe-out pheebs-pheebs drawers] [oven pig aaaaahh marhan death belly-button restore man: tower sweep allo cataract generations be guide hobble beautiful yoko lopped butter it- red way- uh-huuuuh guidance uh-oh woman' moping looked driver phlegm bearings folders drivin' he’s science moans santa reentering he times] goop somewhere mannequins tapanzi mute hair] croissant bonehead mortician band-aid instruction tharp testing moroccas possibility just_ italics present] 'for lunging youth delivering reckless doctor's 2000 cosmo bound reservations soon squinting budolph times fixed mat freshly hopin' cowlicky tissues rings]carol: canadians holy pry ups paint wh- m ship phenomenal [sings] concealed cakes runnin’ leaves][scene: boa blinded barbi notthats-thats pre-game aruba league vell tilly ovulating [smiling] horton soo cancelled score goober ah-ah-ah wait-wait-wait-wait-wait-wait-wait-wait edition like tosses lou disneyland breadsly 'could on bunk agents anchorwoman: “you cracker] horton goldman moms no-one- mustard-tastrophe light fran: mm-hmm i’m closure luggage ummmm blazer] pop-tart carry ya drawer 278 bench\n"
     ]
    }
   ],
   "source": [
    "gen_length = 200\n",
    "# homer_simpson, moe_szyslak, or Barney_Gumble\n",
    "prime_word = 'monica'\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph('./save.meta')\n",
    "    loader.restore(sess, './save')\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    inputs_ = loaded_graph.get_tensor_by_name('inputs:0')\n",
    "    #print (InputTensor)\n",
    "    #input_text, initial_state, final_state, probs\n",
    "    initial_state = loaded_graph.get_tensor_by_name('initial_state:0')\n",
    "    final_state = loaded_graph.get_tensor_by_name('final_state:0')\n",
    "    probs = loaded_graph.get_tensor_by_name('probs:0')\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word + ':']\n",
    "    prev_state = sess.run(initial_state, {inputs_: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[w_to_i[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {inputs_: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        pred_word = pick_word(probabilities[dyn_seq_length-1], i_to_w)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "    print (gen_sentences[:100])\n",
    "    # Remove tokens\n",
    "    tv_script = ' '.join(gen_sentences)\n",
    "    #print (tv_script)\n",
    "    \n",
    "    #print (token_table)    \n",
    "    for key, token in token_table.items():\n",
    "        #print (key)\n",
    "        #print (token)\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        tv_script = tv_script.replace(' ' + token.lower(), key)\n",
    "    tv_script = tv_script.replace('\\n ', '\\n')\n",
    "    tv_script = tv_script.replace('( ', '(')\n",
    "        \n",
    "    print(tv_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monica: layering stirs||period||||right_parentheses||||return||||return||phoebe: ice||period||||return||||return||[scene: also||comma|| neat||period||||return||||return||||return||ross: off||period||||return||phoebe: i've-i've-i've ||left_parentheses||both||return||chandler adults||question_mark|| set||right_parentheses||||return||||return||chandler: answer||period||||right_parentheses|| bob||comma|| mine||exclamation_mark|| i||return||learned hug]||return||||return||chan: distract sophie’s end.a-||return||||return||chandler: ow||exclamation_mark||||exclamation_mark||||exclamation_mark||||exclamation_mark||||exclamation_mark|| shoes||exclamation_mark||||return||||return||ross: too||period||phoebe: curious||semicolon|| open ||left_parentheses||approaching||right_parentheses|| york||period||||return||ross: fridge||right_parentheses||||return||||return||rachel: ‘232 hurts||exclamation_mark||||return||phoebe: ||left_parentheses||intrigued||right_parentheses|| women||exclamation_mark|| chinese]||return||||return||ross: good||period||||return||||return||student: rosellini.||period||||period||||period||as it||return||back||exclamation_mark||||return||ross: is||period||||period||.||left_parentheses||michelle listen||period||||return||[everyone referring owe halfway ride||period||||return||||return||chandler: mucus ||left_parentheses||all left||question_mark||phoebe: monica||period||||return||[cut ||return||||return||||return||||left_parentheses||knock||right_parentheses||||return||||return||||return||chandler: day||period||||return||||return||||return||mnca: minumum yaw||period||||period||||period||carol: job||period||phoebe: no||exclamation_mark||||return||||return||rachel: note||period||||right_parentheses||||return||||return||phoebe: earth handsome||period||||return||||return||mr.aww||exclamation_mark||||return||phoebe: ||return||||left_parentheses||phoebe thinking||period||||right_parentheses||||return||ross: non-breasts||question_mark||||exclamation_mark|| says.||left_parentheses||now grass||period||||return||||return||chandler: vstbl hallowe'en||period||.gallivanting||return||with bean||period||]||return||||return||ross: newborn verbs listen||return||to bing||comma|| y'know||question_mark||||return||||return||phoebe lunging.||return||shots||comma|| season huh||period||||return||||return||||left_parentheses||gunther porn||period||||return||phoebe: perfection'||question_mark|| thumbs||period||||return||phoebe: chachi'||right_parentheses||||return||||return||tv: afteri dragon.right||exclamation_mark||||exclamation_mark||||return||ross: amnesia||period||||return||phoe: paul||exclamation_mark|| [leaves]||return||||return||phoebe: ||left_parentheses||hoshi anyway||question_mark||||return||ross: 'that see||return||him stomach||exclamation_mark||||return||||return||joey: videos bathroom||question_mark|| dan-za||period||||return||||left_parentheses||monica horseback||return||riding||exclamation_mark||||return||phoebe: soupy place||exclamation_mark||||return||phoebe: barber: buying ticking.him||comma||||return||oh||comma|| unbelievable||exclamation_mark|| circle||period||||return||||return||phoebe: bra||period||||return||||return||chandler: tea||return||parties||period||||return||ross: all||question_mark||||return||||return||||return||mr.bunnies||period||||return||||return||phoebe: dinner||question_mark||||return||||return||rachel: ringing||period||||right_parentheses|| shotgun rolling||comma|| out||period||||return||||return||phoebe: counter||period||]||return||||return||rachel: -make eh.out||period||||return||||return||||return||rachel: about||period||||return||||return||monica: sibling||comma|| knows||return||how duck||return||watching crying||period||||right_parentheses||||return||||return||||return||joey: pillleeeee||period||||return||||return||commercial in||exclamation_mark|| honey||period||\n",
      "hate||return||being employees||right_parentheses|| like||return||you the||return||dress||period||||right_parentheses|| city||right_parentheses||||return||||return||rachel: years||period||||return||||return||monica: that||exclamation_mark||||return||[cut choice school||period||||return||phoebe: appalled thinker||period||||return||rachel: original||comma||||return||||return||not kids||period||||return||ross: takes||comma|| changed||exclamation_mark|| lobster||exclamation_mark||||return||||return||monica: monica||right_parentheses||||return||||return||||return||monica: responsibility||period||||period||||period||” job||period||]||return||||return||rtst: world||exclamation_mark||||exclamation_mark|| office||period||||right_parentheses|| alan||right_parentheses||||return||||return||monica: course.tongue||question_mark||||return||||return||ross: better||period||||period||||period||[phoebe ||left_parentheses||honks here||period||||return||||return||||return||mnca: halloween play||period||||return||janice: 1922||question_mark||phoebe: im||return||gonna pleasure||period||||return||||return||phoebe: lie can'twhat applauds grill mitts||period||||right_parentheses||||return||[scene: oberman||period||||return||||return||ross: both||return||clutching knocks||comma|| shes||return||the swallow||period||||return||||return||||return||phoebe: pinching lately||period||||return||fbob: chicken||question_mark||||return||||return||||return||phoebe: rooster||period||||return||phoebe: #2: tissue||question_mark||||return||||return||jade: fruit||period||||return||||return||chan: ||left_parentheses||leaves||right_parentheses||||return||joey: yeah||period||||return||||return||||left_parentheses||he sit||exclamation_mark||||exclamation_mark||||return||||left_parentheses||she laurie you||period||||return||||return||aurora: cake||period||||return||||return||||return||rachel: ok||question_mark||||return||||return||||return||rachel: vacuum.coat||exclamation_mark|| creepers||comma|| hi||period||||return||rach: dollar||period||||return||||return||[scene: bathroom||right_parentheses||||return||||return||chandler: back||period||||right_parentheses||||return||monica: puppets down||period||||right_parentheses||||return||ross: dont hematoma.bedroom the' worried||comma|| special||period||||return||||return||||return||monica: answer||comma||||return||it's froze geisha||period||||return||||return||phoebe: sidestepped shunt\n"
     ]
    }
   ],
   "source": [
    "tv_script = tv_script.replace(' ||return|| ', '\\n')\n",
    "tv_script = tv_script.replace('||period|| ', '.')\n",
    "tv_script = tv_script.replace('|||| ', '.')\n",
    "print(tv_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('.', '||period||'), (';', '||semicolon||'), ('\"', '||quotation_mark||'), ('\\n', '||return||'), ('?', '||question_mark||'), ('--', '||dash||'), ('!', '||exclamation_mark||'), (',', '||comma||'), (')', '||right_parentheses||'), ('(', '||left_parentheses||')])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_table = {'.':'||period||',\n",
    "            ',':'||comma||',\n",
    "           '\"':'||quotation_mark||',\n",
    "           ';': '||semicolon||',\n",
    "           '!': '||exclamation_mark||',\n",
    "           '?': '||question_mark||',\n",
    "           '(': '||left_parentheses||',\n",
    "           ')': '||right_parentheses||',\n",
    "           '--': '||dash||',\n",
    "           '\\n': '||return||'}\n",
    "token_table.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
