{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#from bs4 import BeautifulSoup\n",
    "#from urllib.request import urlopen\n",
    "import numpy as np\n",
    "#import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### get list of pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pages = []\n",
    "for n_1 in range(5):\n",
    "    for n_2 in range (25):\n",
    "        address = 'http://friends.tktv.net/Episodes' + str(n_1+1) +'/summaries/' + str(n_2+1) +'.html'\n",
    "        pages.append(address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "scripts = ''\n",
    "for page in pages:\n",
    "    try:\n",
    "        p = urlopen(page)\n",
    "        p = BeautifulSoup(p,'lxml')\n",
    "        script = p.findAll('body')[0].text\n",
    "        scripts =scripts+script\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "file = open('friends.txt','w')\n",
    "file.write(scripts)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('friends.txt','r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"g\\nSCENE 1: CENTRAL PERK. (ALL PRESENT EXCEPT RACHEL AND ROSS)\\n\\nMONICA: There's nothing to tell! He's just some guy I work with!\\n\\nJOEY: C'mon, you're going out with the guy! There's gotta be something wrong with him!\\n\\nCHANDLER: So does he have a hump? A hump and a hairpiece?\\n\\nPHOEBE: Wait, does he eat chalk?\\n\\n(THE OTHERS STARE, BEMUSED)\\n\\nPHOEBE: Just, 'cause, I don't want her to go through what I went through with Carl- oh!\\n\\nMONICA: Okay, everybody relax. This is not even a date. It's just two people going out to dinner and- not having sex.\\n\\nCHANDLER: Sounds like a date to me.\\n\\n(CUT TO SAME SET)\\n\\nCHANDLER: Alright, so I'm back in high school, I'm standing in the middle of the cafeteria, and I realise I am totally naked.\\n\\nALL: Oh, yeah. Had that dream.\\n\\nCHANDLER: Then I look down, and I realise there's a phone... there.\\n\\nJOEY: Instead of...?\\n\\nCHANDLER: That's right.\\n\\nJOEY: Never had that dream.\\n\\nPHOEBE: No.\\n\\nCHANDLER: All of a sudden, the phone starts to ring. And it turns out it's my mo\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text[81:]\n",
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 33508\n",
      "Number of scenes: 18524\n",
      "Average number of sentences in each scene: 1.0505830274238825\n",
      "Number of lines: 37985\n",
      "Average number of words in each line: 9.371041200473872\n",
      "\n",
      "The sentences 0 to 10:\n",
      "g\n",
      "SCENE 1: CENTRAL PERK. (ALL PRESENT EXCEPT RACHEL AND ROSS)\n",
      "\n",
      "MONICA: There's nothing to tell! He's just some guy I work with!\n",
      "\n",
      "JOEY: C'mon, you're going out with the guy! There's gotta be something wrong with him!\n",
      "\n",
      "CHANDLER: So does he have a hump? A hump and a hairpiece?\n",
      "\n",
      "PHOEBE: Wait, does he eat chalk?\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (0, 10)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "scenes = text.split('\\n\\n')\n",
    "print('Number of scenes: {}'.format(len(scenes)))\n",
    "sentence_count_scene = [scene.count('\\n') for scene in scenes]\n",
    "print('Average number of sentences in each scene: {}'.format(np.average(sentence_count_scene)))\n",
    "\n",
    "sentences = [sentence for scene in scenes for sentence in scene.split('\\n')]\n",
    "print('Number of lines: {}'.format(len(sentences)))\n",
    "word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_sentence)))\n",
    "\n",
    "print()\n",
    "print('The sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "token_table = {'.':'||Period||',\n",
    "            ',':'||Comma||',\n",
    "           '\"':'||Quotation_Mark',\n",
    "           ';': '||Semicolon',\n",
    "           '!': '||Exclamation_mark||',\n",
    "           '?': '||Question_mark||',\n",
    "           '(': '||Left_parentheses',\n",
    "           ')': '||Right_parentheses',\n",
    "           '--': '||Dash||',\n",
    "           '\\n': '||Return||'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for key,token in token_table.items():\n",
    "    text = text.replace(key,' {} '.format(token))\n",
    "text = text.lower()\n",
    "text = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15419"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w_to_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(text)\n",
    "words = sorted(counts, key = counts.get,reverse = True)\n",
    "#words = set(text)\n",
    "w_to_i = {w:i for i,w in enumerate(words)}\n",
    "i_to_w = {i:w for i,w in enumerate(words)}\n",
    "int_text = [w_to_i[w] for w in text]\n",
    "#pickle.dump((int_text,w_to_i,i_to_w,token_table),open('preprocess.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_to_i['||right_parentheses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "embed_dim = 512\n",
    "rnn_size = 512\n",
    "\n",
    "batch_size = 512\n",
    "seq_length = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "515827"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xdata)\n",
    "len(int_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_batch = len(int_text)//(batch_size*seq_length)\n",
    "xdata = int_text[:n_batch*batch_size*seq_length]\n",
    "ydata = int_text[1:n_batch*batch_size*seq_length+1]\n",
    "x_batches = np.split(np.array(xdata).reshape(batch_size,-1),n_batch,1)\n",
    "y_batches = np.split(np.array(ydata).reshape(batch_size,-1),n_batch,1)\n",
    "batches = np.array(list(zip(x_batches,y_batches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batches = batches.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2184,    0,  651, ...,    9,   50,   12],\n",
       "       [   3,  230,  522, ...,    0,    0,   19],\n",
       "       [   0,    0,   21, ...,   25,   20,    2],\n",
       "       ..., \n",
       "       [   5,  146,   44, ...,    2,  189,  167],\n",
       "       [ 268,   74,   15, ...,    1,   12,   62],\n",
       "       [  19,  136, 1144, ...,    6,  166,   68]], dtype=int32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "epoches = 128\n",
    "lr = [0.005]\n",
    "show_every_n_batches = 67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import seq2seq\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(w_to_i)\n",
    "    inputs_ = tf.placeholder(tf.int32,[None,None],name = 'inputs')\n",
    "    targets = tf.placeholder(tf.int32,[None,None],name = 'targets')\n",
    "    learning_rate = tf.placeholder(tf.float32,[None],name = 'learning_rate')\n",
    "    input_data_shape=tf.shape(inputs_)\n",
    "    embedding = tf.Variable(tf.random_uniform([len(w_to_i),embed_dim]))\n",
    "    embed = tf.nn.embedding_lookup(embedding,inputs_)\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm,output_keep_prob = 0.75)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop]*2)\n",
    "    initial_state = tf.identity(cell.zero_state(input_data_shape[0],tf.float32),name = 'initial_state')\n",
    "\n",
    "    output,finalstate = tf.nn.dynamic_rnn(cell,embed,dtype = tf.float32)\n",
    "    final_state = tf.identity(finalstate, name = 'final_state')\n",
    "    logits = tf.contrib.layers.fully_connected(output,vocab_size,activation_fn = None)\n",
    "    \n",
    "    input_data_shape = tf.shape(inputs_)\n",
    "    probs = tf.nn.softmax(logits,name = 'probs')\n",
    "    cost = seq2seq.sequence_loss(logits,targets,tf.ones([input_data_shape[0],input_data_shape[1]]))\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad,-1.,1.),var) for grad,var in gradients]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/67   train_loss = 9.646\n",
      "Epoch   1 Batch    0/67   train_loss = 5.828\n",
      "Epoch   2 Batch    0/67   train_loss = 5.789\n",
      "Epoch   3 Batch    0/67   train_loss = 5.615\n",
      "Epoch   4 Batch    0/67   train_loss = 5.457\n",
      "Epoch   5 Batch    0/67   train_loss = 5.366\n",
      "Epoch   6 Batch    0/67   train_loss = 5.300\n",
      "Epoch   7 Batch    0/67   train_loss = 5.237\n",
      "Epoch   8 Batch    0/67   train_loss = 5.187\n",
      "Epoch   9 Batch    0/67   train_loss = 5.141\n",
      "Epoch  10 Batch    0/67   train_loss = 5.103\n",
      "Epoch  11 Batch    0/67   train_loss = 5.057\n",
      "Epoch  12 Batch    0/67   train_loss = 5.013\n",
      "Epoch  13 Batch    0/67   train_loss = 4.955\n",
      "Epoch  14 Batch    0/67   train_loss = 4.908\n",
      "Epoch  15 Batch    0/67   train_loss = 4.863\n",
      "Epoch  16 Batch    0/67   train_loss = 4.813\n",
      "Epoch  17 Batch    0/67   train_loss = 4.770\n",
      "Epoch  18 Batch    0/67   train_loss = 4.721\n",
      "Epoch  19 Batch    0/67   train_loss = 4.677\n",
      "Epoch  20 Batch    0/67   train_loss = 4.627\n",
      "Epoch  21 Batch    0/67   train_loss = 4.581\n",
      "Epoch  22 Batch    0/67   train_loss = 4.524\n",
      "Epoch  23 Batch    0/67   train_loss = 4.478\n",
      "Epoch  24 Batch    0/67   train_loss = 4.422\n",
      "Epoch  25 Batch    0/67   train_loss = 4.371\n",
      "Epoch  26 Batch    0/67   train_loss = 4.337\n",
      "Epoch  27 Batch    0/67   train_loss = 4.287\n",
      "Epoch  28 Batch    0/67   train_loss = 4.239\n",
      "Epoch  29 Batch    0/67   train_loss = 4.191\n",
      "Epoch  30 Batch    0/67   train_loss = 4.153\n",
      "Epoch  31 Batch    0/67   train_loss = 4.095\n",
      "Epoch  32 Batch    0/67   train_loss = 4.059\n",
      "Epoch  33 Batch    0/67   train_loss = 4.000\n",
      "Epoch  34 Batch    0/67   train_loss = 3.963\n",
      "Epoch  35 Batch    0/67   train_loss = 3.922\n",
      "Epoch  36 Batch    0/67   train_loss = 3.888\n",
      "Epoch  37 Batch    0/67   train_loss = 3.857\n",
      "Epoch  38 Batch    0/67   train_loss = 3.826\n",
      "Epoch  39 Batch    0/67   train_loss = 3.784\n",
      "Epoch  40 Batch    0/67   train_loss = 3.743\n",
      "Epoch  41 Batch    0/67   train_loss = 3.705\n",
      "Epoch  42 Batch    0/67   train_loss = 3.671\n",
      "Epoch  43 Batch    0/67   train_loss = 3.633\n",
      "Epoch  44 Batch    0/67   train_loss = 3.598\n",
      "Epoch  45 Batch    0/67   train_loss = 3.571\n",
      "Epoch  46 Batch    0/67   train_loss = 3.545\n",
      "Epoch  47 Batch    0/67   train_loss = 3.533\n",
      "Epoch  48 Batch    0/67   train_loss = 3.501\n",
      "Epoch  49 Batch    0/67   train_loss = 3.469\n",
      "Epoch  50 Batch    0/67   train_loss = 3.444\n",
      "Epoch  51 Batch    0/67   train_loss = 3.419\n",
      "Epoch  52 Batch    0/67   train_loss = 3.397\n",
      "Epoch  53 Batch    0/67   train_loss = 3.382\n",
      "Epoch  54 Batch    0/67   train_loss = 3.355\n",
      "Epoch  55 Batch    0/67   train_loss = 3.319\n",
      "Epoch  56 Batch    0/67   train_loss = 3.295\n",
      "Epoch  57 Batch    0/67   train_loss = 3.265\n",
      "Epoch  58 Batch    0/67   train_loss = 3.251\n",
      "Epoch  59 Batch    0/67   train_loss = 3.219\n",
      "Epoch  60 Batch    0/67   train_loss = 3.220\n",
      "Epoch  61 Batch    0/67   train_loss = 3.196\n",
      "Epoch  62 Batch    0/67   train_loss = 3.164\n",
      "Epoch  63 Batch    0/67   train_loss = 3.147\n",
      "Epoch  64 Batch    0/67   train_loss = 3.136\n",
      "Epoch  65 Batch    0/67   train_loss = 3.114\n",
      "Epoch  66 Batch    0/67   train_loss = 3.096\n",
      "Epoch  67 Batch    0/67   train_loss = 3.083\n",
      "Epoch  68 Batch    0/67   train_loss = 3.056\n",
      "Epoch  69 Batch    0/67   train_loss = 3.044\n",
      "Epoch  70 Batch    0/67   train_loss = 3.020\n",
      "Epoch  71 Batch    0/67   train_loss = 3.017\n",
      "Epoch  72 Batch    0/67   train_loss = 3.006\n",
      "Epoch  73 Batch    0/67   train_loss = 2.993\n",
      "Epoch  74 Batch    0/67   train_loss = 2.968\n",
      "Epoch  75 Batch    0/67   train_loss = 2.955\n",
      "Epoch  76 Batch    0/67   train_loss = 2.958\n",
      "Epoch  77 Batch    0/67   train_loss = 2.936\n",
      "Epoch  78 Batch    0/67   train_loss = 2.935\n",
      "Epoch  79 Batch    0/67   train_loss = 2.914\n",
      "Epoch  80 Batch    0/67   train_loss = 2.897\n",
      "Epoch  81 Batch    0/67   train_loss = 2.884\n",
      "Epoch  82 Batch    0/67   train_loss = 2.872\n",
      "Epoch  83 Batch    0/67   train_loss = 2.866\n",
      "Epoch  84 Batch    0/67   train_loss = 2.862\n",
      "Epoch  85 Batch    0/67   train_loss = 2.848\n",
      "Epoch  86 Batch    0/67   train_loss = 2.833\n",
      "Epoch  87 Batch    0/67   train_loss = 2.813\n",
      "Epoch  88 Batch    0/67   train_loss = 2.805\n",
      "Epoch  89 Batch    0/67   train_loss = 2.793\n",
      "Epoch  90 Batch    0/67   train_loss = 2.787\n",
      "Epoch  91 Batch    0/67   train_loss = 2.773\n",
      "Epoch  92 Batch    0/67   train_loss = 2.790\n",
      "Epoch  93 Batch    0/67   train_loss = 2.757\n",
      "Epoch  94 Batch    0/67   train_loss = 2.769\n",
      "Epoch  95 Batch    0/67   train_loss = 2.759\n",
      "Epoch  96 Batch    0/67   train_loss = 2.748\n",
      "Epoch  97 Batch    0/67   train_loss = 2.730\n",
      "Epoch  98 Batch    0/67   train_loss = 2.722\n",
      "Epoch  99 Batch    0/67   train_loss = 2.729\n",
      "Epoch 100 Batch    0/67   train_loss = 2.714\n",
      "Epoch 101 Batch    0/67   train_loss = 2.700\n",
      "Epoch 102 Batch    0/67   train_loss = 2.682\n",
      "Epoch 103 Batch    0/67   train_loss = 2.661\n",
      "Epoch 104 Batch    0/67   train_loss = 2.658\n",
      "Epoch 105 Batch    0/67   train_loss = 2.655\n",
      "Epoch 106 Batch    0/67   train_loss = 2.633\n",
      "Epoch 107 Batch    0/67   train_loss = 2.618\n",
      "Epoch 108 Batch    0/67   train_loss = 2.611\n",
      "Epoch 109 Batch    0/67   train_loss = 2.592\n",
      "Epoch 110 Batch    0/67   train_loss = 2.583\n",
      "Epoch 111 Batch    0/67   train_loss = 2.576\n",
      "Epoch 112 Batch    0/67   train_loss = 2.565\n",
      "Epoch 113 Batch    0/67   train_loss = 2.551\n",
      "Epoch 114 Batch    0/67   train_loss = 2.527\n",
      "Epoch 115 Batch    0/67   train_loss = 2.546\n",
      "Epoch 116 Batch    0/67   train_loss = 2.532\n",
      "Epoch 117 Batch    0/67   train_loss = 2.512\n",
      "Epoch 118 Batch    0/67   train_loss = 2.506\n",
      "Epoch 119 Batch    0/67   train_loss = 2.497\n",
      "Epoch 120 Batch    0/67   train_loss = 2.493\n",
      "Epoch 121 Batch    0/67   train_loss = 2.488\n",
      "Epoch 122 Batch    0/67   train_loss = 2.467\n",
      "Epoch 123 Batch    0/67   train_loss = 2.471\n",
      "Epoch 124 Batch    0/67   train_loss = 2.476\n",
      "Epoch 125 Batch    0/67   train_loss = 2.494\n",
      "Epoch 126 Batch    0/67   train_loss = 2.471\n",
      "Epoch 127 Batch    0/67   train_loss = 2.463\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph = train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(epoches):\n",
    "        state = sess.run(initial_state,{inputs_:batches[0][0]})\n",
    "        for batch,(x,y) in enumerate(batches):\n",
    "            feed = {inputs_:x,targets:y,initial_state:state,learning_rate:lr}\n",
    "            train_loss,state,_ = sess.run([cost,final_state,train_op],feed)\n",
    "            if (epoch * len(batches) + batch) % show_every_n_batches == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(epoch,\n",
    "                                                                                batch,\n",
    "                                                                                len(batches),\n",
    "                                                                                train_loss))\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess,'./save')            \n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#pickle.dump((seq_length, './save'), open('params.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    w = np.random.choice(list(int_to_vocab.values()),1,p=probabilities)[0]\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joey: why?!\n",
      "\n",
      "joey:(shyly) i'm not gonna give a dinosaur more outfit to be the whole time.\n",
      "joey: and he's not the happy.\n",
      "ross: what are you have me in? chan: well i ran over there comes in! i\n",
      "hope she was all mad like we did, i think i thought to ruin that i have made\n",
      "his time\n",
      "than if you have to use one of those,\n",
      "(ross) hey!\"\n",
      "mr. burgin: oh, hey, that's totally worse! why dont you do anything?(the camera, the feeling they call explain plays the sleep.)\n",
      "[scene: chandler and joey's, chandler is watching the tables. ]\n",
      "chandler:, are you actually\n",
      "\n",
      "ross: hey why, i'm guessing i like her a bra! hey, everyone was\n",
      "sophie, and ross and\n",
      "chandler: were doing it aren't a wonderful lesbian.\n",
      "monica: oh, uh, isnt it, i'll just call those little cookies at work.(you work into other room, and turns off it points.) come on department. cool phone, all right, don’t be great! it will be very bad,(checks) give us to a stripper.\n",
      "\n",
      "joey: great.(shakes) it doesn’t necessarily! you know?\n",
      "don’t ya realize you stick in.(chandler stops him and) oh...\n",
      "rachel: guys, i was wondering if that’s a guy still in the basement of ross.\n",
      "\n",
      "joey: anyway, let's a cowboy time to somebody’s help you, they're pretty bad.\n",
      "\n",
      "\n",
      "ross: i do not say your mother in your apartment. it's not a one more.\n",
      "\n",
      "\n",
      "phoebe: i can't believe because people don't know if it's happened to be wet the, look, and for those boxes.)\n",
      "\n",
      "chandler:(to chandler) wanna, what are us doing for girls?\n",
      "emily: oh no, he stabbed his eye!\n",
      "[scene: monica and rachel's apartment. phoebe is sitting, carrying sweat of him. ]\n",
      "\n",
      "mrs. bing. ]\n",
      "\n",
      "chandler: y'know, probably kids monica forgot that, time i feel pretty. and after the way i could pick to let a little girl\n",
      "like it.\n",
      "(she slides the bra holder into the door.)\n",
      "rachel: rachel? if i probably dont have to talk his day? did you order a bone?\n",
      "\n",
      "ross: somebody checked his food tonight?\n",
      "\n",
      "chandler: oh, all right.(to intercom) he have bloated?\n",
      "[monica is shocked gets coffee.)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "gen_length = 500\n",
    "prime_word = 'joey'\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    loader = tf.train.import_meta_graph('./save.meta')\n",
    "    loader.restore(sess,'./save')\n",
    "    inputs_ = loaded_graph.get_tensor_by_name('inputs:0')\n",
    "    #print (InputTensor)\n",
    "    #input_text, initial_state, final_state, probs\n",
    "    initial_state = loaded_graph.get_tensor_by_name('initial_state:0')\n",
    "    final_state = loaded_graph.get_tensor_by_name('final_state:0')\n",
    "    probs = loaded_graph.get_tensor_by_name('probs:0')\n",
    "    gen_sentences = [prime_word + ':']\n",
    "    prev_state = sess.run(initial_state, {inputs_: np.array([[1]])})\n",
    "    for n in range(gen_length):\n",
    "        \n",
    "        dyn_input = [[w_to_i[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {inputs_: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        pred_word = pick_word(probabilities[dyn_seq_length-1], i_to_w)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "    #print (gen_sentences[:50])\n",
    "    tv_script = ' '.join(gen_sentences)\n",
    "    for key, token in token_table.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        \n",
    "        tv_script = tv_script.replace(' ' + token.lower(), key)\n",
    "    tv_script = tv_script.replace('\\n ', '\\n')\n",
    "    tv_script = tv_script.replace('( ', '(')\n",
    "        \n",
    "       \n",
    "    print(tv_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
